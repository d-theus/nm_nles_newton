\documentclass[a4paper, 12pt]{article}
%pack{{{
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
%}}}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
\begin{document}
\section*{Титульный лист потерялся}
\newpage
\section*{Задание}
\newpage
\tableofcontents
\newpage
\section*{Введение}
\addcontentsline{toc}{section}{Введение}
\newpage
\section*{Определения}
\addcontentsline{toc}{section}{Определения}
\newpage
\section*{Описание методов}
\addcontentsline{toc}{section}{Описание методов}
Будем рассматривать систему нелинейных уравнений:
\[
\left\{
\begin{array}{ccc}	f_1(x) & = &  0\\
	f_2(x) & = &  0\\
	\ldots \\
	f_n(x) & = &  0 
\end{array}
\text{,}
\right.
\]
где $x = (x_1, x_2 \ldots x_n)$\\
Общая проблема методов решения систем нелинейных уравнений заключается в их сугубо локальном
характере сходимости. Это сильно затрудняет их применение в случаях, когда имеются проблемы с 
выбором начального приближения.\\
Для решения данной проблемы используют численные методы оптимизации, а именно, минимизации.
Необходимо поставить задачу минимизации таким образом, чтобы её приближенное решение являлось решением 
исходной системы нелинейных уравнений. Для этого, можно, например, ввести функцию:
\begin{equation*}
	\Phi(x) = (f_1(x))^2 + (f_2(x))^2 + \ldots + (f_n(x))^2 \text{,}
\end{equation*}
находя минимум которой, найдем и решение исходной системы. 

\subsection*{Метод градиентного спуска}
Из математического анализа известно, что функция растет быстрее всего в направлении
своего градиента. Значит, оптимальным направлением движения для минимизации будет направление, 
противоположное градиенту в данной точке. То есть, 
для нахождения последующего приближения нужно выбирать точку, смещенную относительно предыдущего
приближения на вектор антиградиента с неким коэффициентом, большим нуля.
\begin{equation}
	x^{(k+1)} = x^{(k)} - \alpha \nabla \Phi(x^{(k)}) \text{,}
\end{equation}
где $\alpha$, вообще говоря, зависит от текущего приближения, то есть $\alpha = \alpha_k$
\subsection*{Метод наискорейшего спуска}
Итак, известно направление, в котором функция убывает быстрее всего. Однако, нужно
еще определить, как далеко в этом направлении нужно искать следующее приближение.
А оптимальным этот шаг будет, если значение $\Phi(x^{(k+1)})$ минимальное из всех возможных в этом направлении.
То есть 
\begin{equation}
	\alpha_k = \argmin_{\alpha > 0} (\Phi(x^{(k)} - \alpha \nabla \Phi(x^{(k)})))
\end{equation}
Сходимость этого метода линейная, что медленнее, чем, скажем, у метода Ньютона. Однако, как
говорилось выше, метод Ньютона, как и другие, чувствителен к выбору начального приближения.
Используя на начальном этапе метод наискорейшего спуска можно найти хорошее приближение для него.

\subsection*{Метод Ньютона}
Пусть $A_k$ --- некоторая последовательность невырожденных вещественных $n \times n$-матриц. Тогда, очевидно, последовательность задач
\begin{equation}
	x = x - A_k F(x), k=0,1,2,\ldots
\end{equation}
имеет те же решения, что и исходная система. Для приближенного нахождения этих решений можно формально 
записать итерационный процесс 
\begin{equation}
	x^{(k+1)} = x^{(k)} - A_k F(x^{(k)}), k=0,1,2,\ldots
\end{equation}
Если $$

\begin{equation}
	x^{(k+1)} = x^{(k)} + [F'(x^{(k)})]^{-1} F(x^{(k)})
\end{equation}
\newpage
\section*{Листинг программы}
\addcontentsline{toc}{section}{Листинг программы}
\newpage
\section*{Пример работы}
\addcontentsline{toc}{section}{Пример работы}
\newpage
\section*{Заключение}
\addcontentsline{toc}{section}{Заключение}
\newpage
\end{document}
